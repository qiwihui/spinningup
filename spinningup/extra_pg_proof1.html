

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Extra Material &mdash; Spinning Up  documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/openai_icon.ico"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/modify.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/spinning-up-logo2.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          

            
            
              
            
            
              <p class="caption"><span class="caption-text">用户文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../user/introduction.html">介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/algorithms.html">算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/running.html">运行试验</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/saving_and_loading.html">试验输出</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/plotting.html">绘制结果</a></li>
</ul>
<p class="caption"><span class="caption-text">强化学习介绍</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="rl_intro.html">第一部分：强化学习中的核心概念</a></li>
<li class="toctree-l1"><a class="reference internal" href="rl_intro2.html">第二部分：强化学习算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="rl_intro3.html">第三部分：策略优化介绍</a></li>
</ul>
<p class="caption"><span class="caption-text">资源</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="spinningup.html">Spinning Up as a Deep RL Researcher</a></li>
<li class="toctree-l1"><a class="reference internal" href="keypapers.html">深度强化学习的核心论文</a></li>
<li class="toctree-l1"><a class="reference internal" href="exercises.html">练习</a></li>
<li class="toctree-l1"><a class="reference internal" href="bench.html">Spinning Up 算法实现的基准</a></li>
</ul>
<p class="caption"><span class="caption-text">算法文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/vpg.html">Vanilla Policy Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/trpo.html">Trust Region Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/ppo.html">Proximal Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/ddpg.html">Deep Deterministic Policy Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/td3.html">Twin Delayed DDPG</a></li>
<li class="toctree-l1"><a class="reference internal" href="../algorithms/sac.html">Soft Actor-Critic</a></li>
</ul>
<p class="caption"><span class="caption-text">工具文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../utils/logger.html">日志记录</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/plotter.html">绘图</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/mpi.html">MPI工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/run_utils.html">运行工具</a></li>
</ul>
<p class="caption"><span class="caption-text">其他</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../etc/acknowledgements.html">致谢</a></li>
<li class="toctree-l1"><a class="reference internal" href="../etc/author.html">关于作者</a></li>
</ul>

            
          
<style>
.adsbygoogle-style {
  background-color: #404452;
  margin-top: 20px;
}
</style>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- square-ad -->
<ins class="adsbygoogle adsbygoogle-style"
     style="display:block"
     data-ad-client="ca-pub-8935595858652656"
     data-ad-slot="2636787302"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Spinning Up</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Extra Material</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/spinningup/extra_pg_proof1.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="extra-material">
<h1>Extra Material<a class="headerlink" href="#extra-material" title="Permalink to this headline">¶</a></h1>
<div class="section" id="proof-for-don-t-let-the-past-distract-you">
<h2>Proof for Don&#8217;t Let the Past Distract You<a class="headerlink" href="#proof-for-don-t-let-the-past-distract-you" title="Permalink to this headline">¶</a></h2>
<p>In this subsection, we will prove that actions should not be reinforced for rewards obtained in the past.</p>
<p>Expand out <span class="math">R(\tau)</span> in the expression for the <a class="reference external" href="../spinningup/rl_intro3.html#deriving-the-simplest-policy-gradient">simplest policy gradient</a> to obtain:</p>
<div class="math">
<p><span class="math">\nabla_{\theta} J(\pi_{\theta}) &amp;= \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) R(\tau)} \\
&amp;= \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \sum_{t'=0}^T R(s_{t'}, a_{t'}, s_{t'+1})} \\
&amp;= \sum_{t=0}^{T} \sum_{t'=0}^T  \underE{\tau \sim \pi_{\theta}}{\nabla_{\theta} \log \pi_{\theta}(a_t |s_t) R(s_{t'}, a_{t'}, s_{t'+1})},</span></p>
</div><p>and consider the term</p>
<div class="math">
<p><span class="math">\underE{\tau \sim \pi_{\theta}}{f(t,t')} = \underE{\tau \sim \pi_{\theta}}{\nabla_{\theta} \log \pi_{\theta}(a_t |s_t) R(s_{t'}, a_{t'}, s_{t'+1})}.</span></p>
</div><p>We will show that for the case of <span class="math">t' &lt; t</span> (the reward comes before the action being reinforced), this term is zero. This is a complete proof of the original claim, because after dropping terms with <span class="math">t' &lt; t</span> from the expression, we are left with the reward-to-go form of the policy gradient, as desired:</p>
<div class="math">
<p><span class="math">\nabla_{\theta} J(\pi_{\theta}) = \underE{\tau \sim \pi_{\theta}}{\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \sum_{t'=t}^T R(s_{t'}, a_{t'}, s_{t'+1})}</span></p>
</div><p><strong>1. Using the Marginal Distribution.</strong> To proceed, we have to break down the expectation in <span class="math">\underE{\tau \sim \pi_{\theta}}{f(t,t')}</span>. It&#8217;s an expectation over trajectories, but the expression inside the expectation only deals with a few states and actions: <span class="math">s_t</span>, <span class="math">a_t</span>, <span class="math">s_{t'}</span>, <span class="math">a_{t'}</span>, and <span class="math">s_{t'+1}</span>. So in computing the expectation, we only need to worry about the <a class="reference external" href="https://en.wikipedia.org/wiki/Marginal_distribution">marginal distribution</a> over these random variables.</p>
<p>We derive:</p>
<div class="math">
<p><span class="math">\underE{\tau \sim \pi_{\theta}}{f(t,t')} &amp;= \int_{\tau} P(\tau|\pi_{\theta}) f(t,t') \\
&amp;= \int_{s_t, a_t, s_{t'}, a_{t'}, s_{t'+1}} P(s_t, a_t, s_{t'}, a_{t'}, s_{t'+1} | \pi_{\theta}) f(t,t') \\
&amp;= \underE{s_t, a_t, s_{t'}, a_{t'}, s_{t'+1} \sim \pi_{\theta}}{f(t,t')}.</span></p>
</div><p><strong>2. Probability Chain Rule.</strong> Joint distributions can be calculated in terms of conditional and marginal probabilities via <a class="reference external" href="https://en.wikipedia.org/wiki/Chain_rule_(probability)">chain rule of probability</a>: <span class="math">P(A,B) = P(B|A) P(A)</span>. Here, we use this rule to compute</p>
<div class="math">
<p><span class="math">P(s_t, a_t, s_{t'}, a_{t'}, s_{t'+1} | \pi_{\theta}) = P(s_t, a_t | \pi_{\theta}, s_{t'}, a_{t'}, s_{t'+1}) P(s_{t'}, a_{t'}, s_{t'+1} | \pi_{\theta})</span></p>
</div><p><strong>3. Separating Expectations Over Multiple Random Variables.</strong> If we have an expectation over two random variables <span class="math">A</span> and <span class="math">B</span>, we can split it into an inner and outer expectation, where the inner expectation treats the variable from the outer expectation as a constant. Our ability to make this split relies on probability chain rule. Mathematically:</p>
<div class="math">
<p><span class="math">\underE{A,B}{f(A,B)} &amp;= \int_{A,B} P(A,B) f(A,B) \\
&amp;= \int_{A} \int_B P(B|A) P(A) f(A,B) \\
&amp;= \int_A P(A) \int_B P(B|A) f(A,B) \\
&amp;= \int_A P(A) \underE{B}{f(A,B) \Big| A} \\
&amp;= \underE{A}{\underE{B}{f(A,B) \Big| A} }</span></p>
</div><p>An expectation over <span class="math">s_t, a_t, s_{t'}, a_{t'}, s_{t'+1}</span> can thus be expressed by</p>
<div class="math">
<p><span class="math">\underE{\tau \sim \pi_{\theta}}{f(t,t')} &amp;= \underE{s_t, a_t, s_{t'}, a_{t'}, s_{t'+1} \sim \pi_{\theta}}{f(t,t')} \\
&amp;= \underE{s_{t'}, a_{t'}, s_{t'+1} \sim \pi_{\theta}}{\underE{s_t, a_t \sim \pi_{\theta}}{f(t,t') \Big| s_{t'}, a_{t'}, s_{t'+1}}}</span></p>
</div><p><strong>4. Constants Can Be Pulled Outside of Expectations.</strong> If a term inside an expectation is constant with respect to the variable being expected over, it can be pulled outside of the expectation. To give an example, consider again an expectation over two random variables <span class="math">A</span> and <span class="math">B</span>, where this time, <span class="math">f(A,B) = h(A) g(B)</span>. Then, using the result from before:</p>
<div class="math">
<p><span class="math">\underE{A,B}{f(A,B)} &amp;= \underE{A}{\underE{B}{f(A,B) \Big| A}} \\
&amp;= \underE{A}{\underE{B}{h(A) g(B) \Big| A}}\\
&amp;= \underE{A}{h(A) \underE{B}{g(B) \Big| A}}.</span></p>
</div><p>The function in our expectation decomposes this way, allowing us to write:</p>
<div class="math">
<p><span class="math">\underE{\tau \sim \pi_{\theta}}{f(t,t')} &amp;= \underE{s_{t'}, a_{t'}, s_{t'+1} \sim \pi_{\theta}}{\underE{s_t, a_t \sim \pi_{\theta}}{f(t,t') \Big| s_{t'}, a_{t'}, s_{t'+1}}} \\
&amp;= \underE{s_{t'}, a_{t'}, s_{t'+1} \sim \pi_{\theta}}{\underE{s_t, a_t \sim \pi_{\theta}}{\nabla_{\theta} \log \pi_{\theta}(a_t |s_t) R(s_{t'}, a_{t'}, s_{t'+1}) \Big| s_{t'}, a_{t'}, s_{t'+1}}} \\
&amp;= \underE{s_{t'}, a_{t'}, s_{t'+1} \sim \pi_{\theta}}{R(s_{t'}, a_{t'}, s_{t'+1})  \underE{s_t, a_t \sim \pi_{\theta}}{\nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \Big| s_{t'}, a_{t'}, s_{t'+1}}}.</span></p>
</div><p><strong>5. Applying the EGLP Lemma.</strong> The last step in our proof relies on the <a class="reference external" href="../spinningup/rl_intro3.html#expected-grad-log-prob-lemma">EGLP lemma</a>. At this point, we will only worry about the innermost expectation,</p>
<div class="math">
<p><span class="math">\underE{s_t, a_t \sim \pi_{\theta}}{\nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \Big| s_{t'}, a_{t'}, s_{t'+1}} = \int_{s_t, a_t} P(s_t, a_t | \pi_{\theta}, s_{t'}, a_{t'}, s_{t'+1}) \nabla_{\theta} \log \pi_{\theta}(a_t |s_t).</span></p>
</div><p>We now have to make a distinction between two cases: <span class="math">t' &lt; t</span>, the case where the reward happened before the action, and <span class="math">t' \geq t</span>, where it didn&#8217;t.</p>
<p><strong>Case One: Reward Before Action.</strong> If <span class="math">t' &lt; t</span>, then the conditional probabilities for actions at <span class="math">a_t</span> come from the policy:</p>
<div class="math">
<p><span class="math">P(s_t, a_t | \pi_{\theta}, s_{t'}, a_{t'}, s_{t'+1}) &amp;= \pi_{\theta}(a_t | s_t) P(s_t | \pi_{\theta}, s_{t'}, a_{t'}, s_{t'+1}),</span></p>
</div><p>the innermost expectation can be broken down farther into</p>
<div class="math">
<p><span class="math">\underE{s_t, a_t \sim \pi_{\theta}}{\nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \Big| s_{t'}, a_{t'}, s_{t'+1}} &amp;= \int_{s_t, a_t} P(s_t, a_t | \pi_{\theta}, s_{t'}, a_{t'}, s_{t'+1}) \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \\
&amp;= \int_{s_t} P(s_t | \pi_{\theta}, s_{t'}, a_{t'}, s_{t'+1}) \int_{a_t} \pi_{\theta}(a_t | s_t) \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \\
&amp;= \underE{s_t \sim \pi_{\theta}}{ \underE{a_t \sim \pi_{\theta}}{\nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \Big| s_t } \Big| s_{t'}, a_{t'}, s_{t'+1}}.</span></p>
</div><p>The EGLP lemma says that</p>
<div class="math">
<p><span class="math">\underE{a_t \sim \pi_{\theta}}{\nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \Big| s_t } = 0,</span></p>
</div><p>allowing us to conclude that for <span class="math">t' &lt; t</span>, <span class="math">\underE{\tau \sim \pi_{\theta}}{f(t,t')} = 0</span>.</p>
<p><strong>Case Two: Reward After Action.</strong> What about the <span class="math">t' \geq t</span> case, though? Why doesn&#8217;t the same logic apply? In this case, the conditional probabilities for <span class="math">a_t</span> can&#8217;t be broken down the same way, because you&#8217;re conditioning <strong>on the future.</strong> Think about it like this: let&#8217;s say that every day, in the morning, you make a choice between going for a jog and going to work early, and you have a 50-50 chance of each option. If you condition on a future where you went to work early, what are the odds that you went for a jog? Clearly, you didn&#8217;t. But if you&#8217;re conditioning on the past&#8212;before you made the decision&#8212;what are the odds that you will later go for a jog? Now it&#8217;s back to 50-50.</p>
<p>So in the case where <span class="math">t' \geq t</span>, the conditional distribution over actions <span class="math">a_t</span> is <strong>not</strong> <span class="math">\pi(a_t|s_t)</span>, and the EGLP lemma does not apply.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, OpenAI.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-46660488-7"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-46660488-7');
</script>



</body>
</html>