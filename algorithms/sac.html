

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Soft Actor-Critic &mdash; Spinning Up  documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/openai_icon.ico"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/modify.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="日志记录" href="../utils/logger.html" />
    <link rel="prev" title="Twin Delayed DDPG" href="td3.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/spinning-up-logo2.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          

            
            
              
            
            
              <p class="caption"><span class="caption-text">用户文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../user/introduction.html">介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/algorithms.html">算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/running.html">运行试验</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/saving_and_loading.html">试验输出</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/plotting.html">绘制结果</a></li>
</ul>
<p class="caption"><span class="caption-text">强化学习介绍</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/rl_intro.html">第一部分：强化学习中的核心概念</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/rl_intro2.html">第二部分：强化学习算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/rl_intro3.html">第三部分：策略优化介绍</a></li>
</ul>
<p class="caption"><span class="caption-text">资源</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/spinningup.html">深度强化学习研究者资料</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/keypapers.html">深度强化学习的核心论文</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/exercises.html">练习</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/bench.html">Spinning Up 算法实现的基准</a></li>
</ul>
<p class="caption"><span class="caption-text">算法文档</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="vpg.html">Vanilla Policy Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="trpo.html">Trust Region Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="ppo.html">Proximal Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddpg.html">Deep Deterministic Policy Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="td3.html">Twin Delayed DDPG</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Soft Actor-Critic</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id1">背景</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2">速览</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id3">关键方程</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#entropy-regularized-reinforcement-learning">Entropy-Regularized Reinforcement Learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id4">Soft Actor-Critic</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id5">探索与利用</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id6">伪代码</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id7">文档</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id8">保存的模型的内容</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id9">参考</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id10">相关论文</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id11">其他公开实现</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">工具文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../utils/logger.html">日志记录</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/plotter.html">绘图</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/mpi.html">MPI工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/run_utils.html">运行工具</a></li>
</ul>
<p class="caption"><span class="caption-text">其他</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../etc/acknowledgements.html">致谢</a></li>
<li class="toctree-l1"><a class="reference internal" href="../etc/author.html">关于作者</a></li>
</ul>

            
          
<script data-ad-client="ca-pub-8935595858652656" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Spinning Up</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Soft Actor-Critic</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/algorithms/sac.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="soft-actor-critic">
<h1><a class="toc-backref" href="#id12">Soft Actor-Critic</a><a class="headerlink" href="#soft-actor-critic" title="Permalink to this headline">¶</a></h1>
<div class="contents topic" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><a class="reference internal" href="#soft-actor-critic" id="id12">Soft Actor-Critic</a><ul>
<li><a class="reference internal" href="#id1" id="id13">背景</a><ul>
<li><a class="reference internal" href="#id2" id="id14">速览</a></li>
<li><a class="reference internal" href="#id3" id="id15">关键方程</a><ul>
<li><a class="reference internal" href="#entropy-regularized-reinforcement-learning" id="id16">Entropy-Regularized Reinforcement Learning</a></li>
<li><a class="reference internal" href="#id4" id="id17">Soft Actor-Critic</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id5" id="id18">探索与利用</a></li>
<li><a class="reference internal" href="#id6" id="id19">伪代码</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id7" id="id20">文档</a><ul>
<li><a class="reference internal" href="#id8" id="id21">保存的模型的内容</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id9" id="id22">参考</a><ul>
<li><a class="reference internal" href="#id10" id="id23">相关论文</a></li>
<li><a class="reference internal" href="#id11" id="id24">其他公开实现</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="id1">
<h2><a class="toc-backref" href="#id13">背景</a><a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>(前一节 <a class="reference external" href="../algorithms/td3.html#background">背景 for TD3</a>)</p>
<p>Soft Actor Critic (SAC) is an algorithm which optimizes a stochastic policy in an off-policy way, forming a bridge between stochastic policy optimization and DDPG-style approaches. It isn&#8217;t a direct successor to TD3 (having been published roughly concurrently), but it incorporates the clipped double-Q trick, and due to the inherent stochasticity of the policy in SAC, it also winds up benefiting from something like target policy smoothing.</p>
<p>A central feature of SAC is <strong>entropy regularization.</strong> The policy is trained to maximize a trade-off between expected return and <a class="reference external" href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">entropy</a>, a measure of randomness in the policy. This has a close connection to the exploration-exploitation trade-off: increasing entropy results in more exploration, which can accelerate learning later on. It can also prevent the policy from prematurely converging to a bad local optimum.</p>
<div class="section" id="id2">
<h3><a class="toc-backref" href="#id14">速览</a><a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li>SAC is an off-policy algorithm.</li>
<li>The version of SAC implemented here can only be used for environments with continuous action spaces.</li>
<li>An alternate version of SAC, which slightly changes the policy update rule, can be implemented to handle discrete action spaces.</li>
<li>The Spinning Up implementation of SAC does not support parallelization.</li>
</ul>
</div>
<div class="section" id="id3">
<h3><a class="toc-backref" href="#id15">关键方程</a><a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>To explain Soft Actor Critic, we first have to introduce the entropy-regularized reinforcement learning setting. In entropy-regularized RL, there are slightly-different equations for value functions.</p>
<div class="section" id="entropy-regularized-reinforcement-learning">
<h4><a class="toc-backref" href="#id16">Entropy-Regularized Reinforcement Learning</a><a class="headerlink" href="#entropy-regularized-reinforcement-learning" title="Permalink to this headline">¶</a></h4>
<p>Entropy is a quantity which, roughly speaking, says how random a random variable is. If a coin is weighted so that it almost always comes up heads, it has low entropy; if it&#8217;s evenly weighted and has a half chance of either outcome, it has high entropy.</p>
<p>Let <span class="math">x</span> be a random variable with probability mass or density function <span class="math">P</span>. The entropy <span class="math">H</span> of <span class="math">x</span> is computed from its distribution <span class="math">P</span> according to</p>
<div class="math">
<p><span class="math">H(P) = \underE{x \sim P}{-\log P(x)}.</span></p>
</div><p>In entropy-regularized reinforcement learning, the agent gets a bonus reward at each time step proportional to the entropy of the policy at that timestep. This changes <a class="reference external" href="../spinningup/rl_intro.html#the-rl-problem">the RL problem</a> to:</p>
<div class="math">
<p><span class="math">\pi^* = \arg \max_{\pi} \underE{\tau \sim \pi}{ \sum_{t=0}^{\infty} \gamma^t \bigg( R(s_t, a_t, s_{t+1}) + \alpha H\left(\pi(\cdot|s_t)\right) \bigg)},</span></p>
</div><p>where <span class="math">\alpha &gt; 0</span> is the trade-off coefficient. (Note: we&#8217;re assuming an infinite-horizon discounted setting here, and we&#8217;ll do the same for the rest of this page.) We can now define the slightly-different value functions in this setting. <span class="math">V^{\pi}</span> is changed to include the entropy bonuses from every timestep:</p>
<div class="math">
<p><span class="math">V^{\pi}(s) = \underE{\tau \sim \pi}{ \left. \sum_{t=0}^{\infty} \gamma^t \bigg( R(s_t, a_t, s_{t+1}) + \alpha H\left(\pi(\cdot|s_t)\right) \bigg) \right| s_0 = s}</span></p>
</div><p><span class="math">Q^{\pi}</span> is changed to include the entropy bonuses from every timestep <em>except the first</em>:</p>
<div class="math">
<p><span class="math">Q^{\pi}(s,a) = \underE{\tau \sim \pi}{ \left. \sum_{t=0}^{\infty} \gamma^t  R(s_t, a_t, s_{t+1}) + \alpha \sum_{t=1}^{\infty} \gamma^t H\left(\pi(\cdot|s_t)\right)\right| s_0 = s, a_0 = a}</span></p>
</div><p>With these definitions, <span class="math">V^{\pi}</span> and <span class="math">Q^{\pi}</span> are connected by:</p>
<div class="math">
<p><span class="math">V^{\pi}(s) = \underE{a \sim \pi}{Q^{\pi}(s,a)} + \alpha H\left(\pi(\cdot|s)\right)</span></p>
</div><p>and the Bellman equation for <span class="math">Q^{\pi}</span> is</p>
<div class="math">
<p><span class="math">Q^{\pi}(s,a) &amp;= \underE{s' \sim P \\ a' \sim \pi}{R(s,a,s') + \gamma\left(Q^{\pi}(s',a') + \alpha H\left(\pi(\cdot|s')\right) \right)} \\
&amp;= \underE{s' \sim P}{R(s,a,s') + \gamma V^{\pi}(s')}.</span></p>
</div><div class="admonition- admonition">
<p class="first admonition-title">你应该知道</p>
<p class="last">The way we&#8217;ve set up the value functions in the entropy-regularized setting is a little bit arbitrary, and actually we could have done it differently (eg make <span class="math">Q^{\pi}</span> include the entropy bonus at the first timestep). The choice of definition may vary slightly across papers on the subject.</p>
</div>
</div>
<div class="section" id="id4">
<h4><a class="toc-backref" href="#id17">Soft Actor-Critic</a><a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h4>
<p>SAC concurrently learns a policy <span class="math">\pi_{\theta}</span>, two Q-functions <span class="math">Q_{\phi_1}, Q_{\phi_2}</span>, and a value function <span class="math">V_{\psi}</span>.</p>
<p><strong>Learning Q.</strong> The Q-functions are learned by MSBE minimization, using a <strong>target value network</strong> to form the Bellman backups. They both use the same target, like in TD3, and have loss functions:</p>
<div class="math">
<p><span class="math">L(\phi_i, {\mathcal D}) = \underset{(s,a,r,s',d) \sim {\mathcal D}}{{\mathrm E}}\left[
    \Bigg( Q_{\phi_i}(s,a) - \left(r + \gamma (1 - d) V_{\psi_{\text{targ}}}(s') \right) \Bigg)^2
    \right].</span></p>
</div><p>The target value network, like the target networks in DDPG and TD3, is obtained by polyak averaging the value network parameters over the course of training.</p>
<p><strong>Learning V.</strong> The value function is learned by exploiting (a sample-based approximation of) the connection between <span class="math">Q^{\pi}</span> and <span class="math">V^{\pi}</span>. Before we go into the learning rule, let&#8217;s first rewrite the connection equation by using the definition of entropy to obtain:</p>
<div class="math">
<p><span class="math">V^{\pi}(s) &amp;= \underE{a \sim \pi}{Q^{\pi}(s,a)} + \alpha H\left(\pi(\cdot|s)\right) \\
&amp;= \underE{a \sim \pi}{Q^{\pi}(s,a) - \alpha \log \pi(a|s)}.</span></p>
</div><p>The RHS is an expectation over actions, so we can approximate it by sampling from the policy:</p>
<div class="math">
<p><span class="math">V^{\pi}(s) \approx Q^{\pi}(s,\tilde{a}) - \alpha \log \pi(\tilde{a}|s), \;\;\;\;\; \tilde{a} \sim \pi(\cdot|s).</span></p>
</div><p>SAC sets up a mean-squared-error loss for <span class="math">V_{\psi}</span> based on this approximation. But what Q-value do we use? SAC uses <strong>clipped double-Q</strong> like TD3 for learning the value function, and takes the minimum Q-value between the two approximators. So the SAC loss for value function parameters is:</p>
<div class="math">
<p><span class="math">L(\psi, {\mathcal D}) = \underE{s \sim \mathcal{D} \\ \tilde{a} \sim \pi_{\theta}}{\Bigg(V_{\psi}(s) - \left(\min_{i=1,2} Q_{\phi_i}(s,\tilde{a}) - \alpha \log \pi_{\theta}(\tilde{a}|s) \right)\Bigg)^2}.</span></p>
</div><p>Importantly, we do <strong>not</strong> use actions from the replay buffer here: these actions are sampled fresh from the current version of the policy.</p>
<p><strong>Learning the Policy.</strong> The policy should, in each state, act to maximize the expected future return plus expected future entropy. That is, it should maximize <span class="math">V^{\pi}(s)</span>, which we expand out (as before) into</p>
<div class="math">
<p><span class="math">\underE{a \sim \pi}{Q^{\pi}(s,a) - \alpha \log \pi(a|s)}.</span></p>
</div><p>The way we optimize the policy makes use of the <strong>reparameterization trick</strong>, in which a sample from <span class="math">\pi_{\theta}(\cdot|s)</span> is drawn by computing a deterministic function of state, policy parameters, and independent noise. To illustrate: following the authors of the SAC paper, we use a squashed Gaussian policy, which means that samples are obtained according to</p>
<div class="math">
<p><span class="math">\tilde{a}_{\theta}(s, \xi) = \tanh\left( \mu_{\theta}(s) + \sigma_{\theta}(s) \odot \xi \right), \;\;\;\;\; \xi \sim \mathcal{N}(0, I).</span></p>
</div><div class="admonition- admonition">
<p class="first admonition-title">你应该知道</p>
<p>This policy has two key differences from the policies we use in the other policy optimization algorithms:</p>
<p><strong>1. The squashing function.</strong> The <span class="math">\tanh</span> in the SAC policy ensures that actions are bounded to a finite range. This is absent in the VPG, TRPO, and PPO policies. It also changes the distribution: before the <span class="math">\tanh</span> the SAC policy is a factored Gaussian like the other algorithms&#8217; policies, but after the <span class="math">\tanh</span> it is not. (You can still compute the log-probabilities of actions in closed form, though: see the paper appendix for details.)</p>
<p class="last"><strong>2. The way standard deviations are parameterized.</strong> In VPG, TRPO, and PPO, we represent the log std devs with state-independent parameter vectors. In SAC, we represent the log std devs as outputs from the neural network, meaning that they depend on state in a complex way. SAC with state-independent log std devs, in our experience, did not work. (Can you think of why? Or better yet: run an experiment to verify?)</p>
</div>
<p>The reparameterization trick allows us to rewrite the expectation over actions (which contains a pain point: the distribution depends on the policy parameters) into an expectation over noise (which removes the pain point: the distribution now has no dependence on parameters):</p>
<div class="math">
<p><span class="math">\underE{a \sim \pi_{\theta}}{Q^{\pi_{\theta}}(s,a) - \alpha \log \pi_{\theta}(a|s)} = \underE{\xi \sim \mathcal{N}}{Q^{\pi_{\theta}}(s,\tilde{a}_{\theta}(s,\xi)) - \alpha \log \pi_{\theta}(\tilde{a}_{\theta}(s,\xi)|s)}</span></p>
</div><p>To get the policy loss, the final step is that we need to substitute <span class="math">Q^{\pi_{\theta}}</span> with one of our function approximators. The same as in TD3, we use <span class="math">Q_{\phi_1}</span>. The policy is thus optimized according to</p>
<div class="math">
<p><span class="math">\max_{\theta} \underE{s \sim \mathcal{D} \\ \xi \sim \mathcal{N}}{Q_{\phi_1}(s,\tilde{a}_{\theta}(s,\xi)) - \alpha \log \pi_{\theta}(\tilde{a}_{\theta}(s,\xi)|s)},</span></p>
</div><p>which is almost the same as the DDPG and TD3 policy optimization, except for the stochasticity and entropy term.</p>
</div>
</div>
<div class="section" id="id5">
<h3><a class="toc-backref" href="#id18">探索与利用</a><a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>SAC trains a stochastic policy with entropy regularization, and explores in an on-policy way. The entropy regularization coefficient <span class="math">\alpha</span> explicitly controls the explore-exploit tradeoff, with higher <span class="math">\alpha</span> corresponding to more exploration, and lower <span class="math">\alpha</span> corresponding to more exploitation. The right coefficient (the one which leads to the stablest / highest-reward learning) may vary from environment to environment, and could require careful tuning.</p>
<p>At test time, to see how well the policy exploits what it has learned, we remove stochasticity and use the mean action instead of a sample from the distribution. This tends to improve performance over the original stochastic policy.</p>
<div class="admonition- admonition">
<p class="first admonition-title">你应该知道</p>
<p class="last">Our SAC implementation uses a trick to improve exploration at the start of training. For a fixed number of steps at the beginning (set with the <code class="docutils literal"><span class="pre">start_steps</span></code> keyword argument), the agent takes actions which are sampled from a uniform random distribution over valid actions. After that, it returns to normal SAC exploration.</p>
</div>
</div>
<div class="section" id="id6">
<h3><a class="toc-backref" href="#id19">伪代码</a><a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<div class="math">
<p><span class="math">\begin{algorithm}[H]
    \caption{Soft Actor-Critic}
    \label{alg1}
\begin{algorithmic}[1]
    \STATE Input: initial policy parameters $\theta$, Q-function parameters $\phi_1$, $\phi_2$, V-function parameters $\psi$, empty replay buffer $\mathcal{D}$
    \STATE Set target parameters equal to main parameters $\psi_{\text{targ}} \leftarrow \psi$
    \REPEAT
        \STATE Observe state $s$ and select action $a \sim \pi_{\theta}(\cdot|s)$
        \STATE Execute $a$ in the environment
        \STATE Observe next state $s'$, reward $r$, and done signal $d$ to indicate whether $s'$ is terminal
        \STATE Store $(s,a,r,s',d)$ in replay buffer $\mathcal{D}$
        \STATE If $s'$ is terminal, reset environment state.
        \IF{it's time to update}
            \FOR{$j$ in range(however many updates)}
                \STATE Randomly sample a batch of transitions, $B = \{ (s,a,r,s',d) \}$ from $\mathcal{D}$
                \STATE Compute targets for Q and V functions:
                \begin{align*}
                    y_q (r,s',d) &amp;= r + \gamma (1-d) V_{\psi_{\text{targ}}}(s') &amp;&amp;\\
                    y_v (s) &amp;= \min_{i=1,2} Q_{\phi_i} (s, \tilde{a}) - \alpha \log \pi_{\theta}(\tilde{a}|s), &amp;&amp; \tilde{a} \sim \pi_{\theta}(\cdot|s)
                \end{align*}
                \STATE Update Q-functions by one step of gradient descent using
                \begin{align*}
                    &amp; \nabla_{\phi_i} \frac{1}{|B|}\sum_{(s,a,r,s',d) \in B} \left( Q_{\phi,i}(s,a) - y_q(r,s',d) \right)^2 &amp;&amp; \text{for } i=1,2
                \end{align*}
                \STATE Update V-function by one step of gradient descent using
                \begin{equation*}
                    \nabla_{\psi} \frac{1}{|B|}\sum_{s \in B} \left( V_{\psi}(s) - y_v(s) \right)^2
                \end{equation*}
                \STATE Update policy by one step of gradient ascent using
                \begin{equation*}
                    \nabla_{\theta} \frac{1}{|B|}\sum_{s \in B} \Big( Q_{\phi,1}(s, \tilde{a}_{\theta}(s)) - \alpha \log \pi_{\theta} \left(\left. \tilde{a}_{\theta}(s) \right| s\right) \Big),
                \end{equation*}
                where $\tilde{a}_{\theta}(s)$ is a sample from $\pi_{\theta}(\cdot|s)$ which is differentiable wrt $\theta$ via the reparametrization trick.
                \STATE Update target value network with
                \begin{align*}
                    \psi_{\text{targ}} &amp;\leftarrow \rho \psi_{\text{targ}} + (1-\rho) \psi
                \end{align*}
            \ENDFOR
        \ENDIF
    \UNTIL{convergence}
\end{algorithmic}
\end{algorithm}</span></p>
</div></div>
</div>
<div class="section" id="id7">
<h2><a class="toc-backref" href="#id20">文档</a><a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="spinup.sac">
<code class="descclassname">spinup.</code><code class="descname">sac</code><span class="sig-paren">(</span><em>env_fn</em>, <em>actor_critic=&lt;function mlp_actor_critic&gt;</em>, <em>ac_kwargs={}</em>, <em>seed=0</em>, <em>steps_per_epoch=5000</em>, <em>epochs=100</em>, <em>replay_size=1000000</em>, <em>gamma=0.99</em>, <em>polyak=0.995</em>, <em>lr=0.001</em>, <em>alpha=0.2</em>, <em>batch_size=100</em>, <em>start_steps=10000</em>, <em>max_ep_len=1000</em>, <em>logger_kwargs={}</em>, <em>save_freq=1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/spinup/algos/sac/sac.html#sac"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#spinup.sac" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>env_fn</strong> &#8211; A function which creates a copy of the environment.
The environment must satisfy the OpenAI Gym API.</li>
<li><strong>actor_critic</strong> &#8211; <p>A function which takes in placeholder symbols
for state, <code class="docutils literal"><span class="pre">x_ph</span></code>, and action, <code class="docutils literal"><span class="pre">a_ph</span></code>, and returns the main
outputs from the agent&#8217;s Tensorflow computation graph:</p>
<table border="1" class="docutils">
<colgroup>
<col width="16%" />
<col width="23%" />
<col width="61%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Symbol</th>
<th class="head">Shape</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">mu</span></code></td>
<td>(batch, act_dim)</td>
<td><div class="first last line-block">
<div class="line">Computes mean actions from policy</div>
<div class="line">given states.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">pi</span></code></td>
<td>(batch, act_dim)</td>
<td><div class="first last line-block">
<div class="line">Samples actions from policy given</div>
<div class="line">states.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">logp_pi</span></code></td>
<td>(batch,)</td>
<td><div class="first last line-block">
<div class="line">Gives log probability, according to</div>
<div class="line">the policy, of the action sampled by</div>
<div class="line"><code class="docutils literal"><span class="pre">pi</span></code>. Critical: must be differentiable</div>
<div class="line">with respect to policy parameters all</div>
<div class="line">the way through action sampling.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">q1</span></code></td>
<td>(batch,)</td>
<td><div class="first last line-block">
<div class="line">Gives one estimate of Q* for</div>
<div class="line">states in <code class="docutils literal"><span class="pre">x_ph</span></code> and actions in</div>
<div class="line"><code class="docutils literal"><span class="pre">a_ph</span></code>.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">q2</span></code></td>
<td>(batch,)</td>
<td><div class="first last line-block">
<div class="line">Gives another estimate of Q* for</div>
<div class="line">states in <code class="docutils literal"><span class="pre">x_ph</span></code> and actions in</div>
<div class="line"><code class="docutils literal"><span class="pre">a_ph</span></code>.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">q1_pi</span></code></td>
<td>(batch,)</td>
<td><div class="first last line-block">
<div class="line">Gives the composition of <code class="docutils literal"><span class="pre">q1</span></code> and</div>
<div class="line"><code class="docutils literal"><span class="pre">pi</span></code> for states in <code class="docutils literal"><span class="pre">x_ph</span></code>:</div>
<div class="line">q1(x, pi(x)).</div>
</div>
</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">q2_pi</span></code></td>
<td>(batch,)</td>
<td><div class="first last line-block">
<div class="line">Gives the composition of <code class="docutils literal"><span class="pre">q2</span></code> and</div>
<div class="line"><code class="docutils literal"><span class="pre">pi</span></code> for states in <code class="docutils literal"><span class="pre">x_ph</span></code>:</div>
<div class="line">q2(x, pi(x)).</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">v</span></code></td>
<td>(batch,)</td>
<td><div class="first last line-block">
<div class="line">Gives the value estimate for states</div>
<div class="line">in <code class="docutils literal"><span class="pre">x_ph</span></code>.</div>
</div>
</td>
</tr>
</tbody>
</table>
</li>
<li><strong>ac_kwargs</strong> (<em>dict</em>) &#8211; Any kwargs appropriate for the actor_critic
function you provided to SAC.</li>
<li><strong>seed</strong> (<em>int</em>) &#8211; Seed for random number generators.</li>
<li><strong>steps_per_epoch</strong> (<em>int</em>) &#8211; Number of steps of interaction (state-action pairs)
for the agent and the environment in each epoch.</li>
<li><strong>epochs</strong> (<em>int</em>) &#8211; Number of epochs to run and train agent.</li>
<li><strong>replay_size</strong> (<em>int</em>) &#8211; Maximum length of replay buffer.</li>
<li><strong>gamma</strong> (<em>float</em>) &#8211; Discount factor. (Always between 0 and 1.)</li>
<li><strong>polyak</strong> (<em>float</em>) &#8211; <p>Interpolation factor in polyak averaging for target
networks. Target networks are updated towards main networks
according to:</p>
<div class="math">
<p><span class="math">\theta_{\text{targ}} \leftarrow
\rho \theta_{\text{targ}} + (1-\rho) \theta</span></p>
</div><p>where <span class="math">\rho</span> is polyak. (Always between 0 and 1, usually
close to 1.)</p>
</li>
<li><strong>lr</strong> (<em>float</em>) &#8211; Learning rate (used for both policy and value learning).</li>
<li><strong>alpha</strong> (<em>float</em>) &#8211; Entropy regularization coefficient. (Equivalent to
inverse of reward scale in the original SAC paper.)</li>
<li><strong>batch_size</strong> (<em>int</em>) &#8211; Minibatch size for SGD.</li>
<li><strong>start_steps</strong> (<em>int</em>) &#8211; Number of steps for uniform-random action selection,
before running real policy. Helps exploration.</li>
<li><strong>max_ep_len</strong> (<em>int</em>) &#8211; Maximum length of trajectory / episode / rollout.</li>
<li><strong>logger_kwargs</strong> (<em>dict</em>) &#8211; Keyword args for EpochLogger.</li>
<li><strong>save_freq</strong> (<em>int</em>) &#8211; How often (in terms of gap between epochs) to save
the current policy and value function.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<div class="section" id="id8">
<h3><a class="toc-backref" href="#id21">保存的模型的内容</a><a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<p>记录的计算图包括：</p>
<table border="1" class="docutils">
<colgroup>
<col width="9%" />
<col width="91%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">键</th>
<th class="head">值</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">x</span></code></td>
<td>Tensorflow placeholder for state input.</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">a</span></code></td>
<td>Tensorflow placeholder for action input.</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">mu</span></code></td>
<td>Deterministically computes mean action from the agent, given states in <code class="docutils literal"><span class="pre">x</span></code>.</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">pi</span></code></td>
<td>Samples an action from the agent, conditioned on states in <code class="docutils literal"><span class="pre">x</span></code>.</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">q1</span></code></td>
<td>Gives one action-value estimate for states in <code class="docutils literal"><span class="pre">x</span></code> and actions in <code class="docutils literal"><span class="pre">a</span></code>.</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">q2</span></code></td>
<td>Gives the other action-value estimate for states in <code class="docutils literal"><span class="pre">x</span></code> and actions in <code class="docutils literal"><span class="pre">a</span></code>.</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">v</span></code></td>
<td>Gives the value estimate for states in <code class="docutils literal"><span class="pre">x</span></code>.</td>
</tr>
</tbody>
</table>
<p>可以通过以下方式访问此保存的模型</p>
<ul class="simple">
<li>使用 <a class="reference external" href="../user/saving_and_loading.html#loading-and-running-trained-policies">test_policy.py</a> 工具运行经过训练的策略，</li>
<li>或使用 <a class="reference external" href="../utils/logger.html#spinup.utils.logx.restore_tf_graph">restore_tf_graph</a> 将整个保存的图形加载到程序中。</li>
</ul>
<p>注意：对于SAC，正确的评估策略由 <code class="docutils literal"><span class="pre">mu</span></code> 而不是 <code class="docutils literal"><span class="pre">pi</span></code> 给出。可以将策略 <code class="docutils literal"><span class="pre">pi</span></code> 视为探索策略，而将 <code class="docutils literal"><span class="pre">mu</span></code> 视为开发策略。</p>
</div>
</div>
<div class="section" id="id9">
<h2><a class="toc-backref" href="#id22">参考</a><a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id10">
<h3><a class="toc-backref" href="#id23">相关论文</a><a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="https://arxiv.org/abs/1801.01290">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</a>, Haarnoja et al, 2018</li>
</ul>
</div>
<div class="section" id="id11">
<h3><a class="toc-backref" href="#id24">其他公开实现</a><a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="https://github.com/haarnoja/sac">SAC release repo</a></li>
</ul>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../utils/logger.html" class="btn btn-neutral float-right" title="日志记录" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="td3.html" class="btn btn-neutral" title="Twin Delayed DDPG" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, OpenAI.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-46660488-7"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-46660488-7');
</script>



</body>
</html>