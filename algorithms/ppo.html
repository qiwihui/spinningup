

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Proximal Policy Optimization &mdash; Spinning Up  documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/openai_icon.ico"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/modify.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Deep Deterministic Policy Gradient" href="ddpg.html" />
    <link rel="prev" title="Trust Region Policy Optimization" href="trpo.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/spinning-up-logo2.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          

            
            
              
            
            
              <p class="caption"><span class="caption-text">用户文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../user/introduction.html">介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/algorithms.html">算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/running.html">运行试验</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/saving_and_loading.html">试验输出</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/plotting.html">绘制结果</a></li>
</ul>
<p class="caption"><span class="caption-text">强化学习介绍</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/rl_intro.html">第一部分：强化学习中的核心概念</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/rl_intro2.html">第二部分：强化学习算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/rl_intro3.html">第三部分：策略优化介绍</a></li>
</ul>
<p class="caption"><span class="caption-text">资源</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/spinningup.html">深度强化学习研究者资料</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/keypapers.html">深度强化学习的核心论文</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/exercises.html">练习</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/bench.html">Spinning Up 算法实现的基准</a></li>
</ul>
<p class="caption"><span class="caption-text">算法文档</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="vpg.html">Vanilla Policy Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="trpo.html">Trust Region Policy Optimization</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Proximal Policy Optimization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id1">背景</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2">速览</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id3">关键方程</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id7">探索与利用</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id8">伪代码</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id9">文档</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id10">保存的模型的内容</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id11">参考</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id12">相关论文</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id13">为什么是这些论文？</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id14">其他公开实现</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ddpg.html">Deep Deterministic Policy Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="td3.html">Twin Delayed DDPG</a></li>
<li class="toctree-l1"><a class="reference internal" href="sac.html">Soft Actor-Critic</a></li>
</ul>
<p class="caption"><span class="caption-text">工具文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../utils/logger.html">日志记录</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/plotter.html">绘图</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/mpi.html">MPI工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/run_utils.html">运行工具</a></li>
</ul>
<p class="caption"><span class="caption-text">其他</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../etc/acknowledgements.html">致谢</a></li>
<li class="toctree-l1"><a class="reference internal" href="../etc/author.html">关于作者</a></li>
</ul>

            
          
<style>
.adsbygoogle-style {
  background-color: #404452;
  margin-top: 20px;
}
</style>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- square-ad -->
<ins class="adsbygoogle adsbygoogle-style"
     style="display:block"
     data-ad-client="ca-pub-8935595858652656"
     data-ad-slot="2636787302"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Spinning Up</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Proximal Policy Optimization</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/algorithms/ppo.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="proximal-policy-optimization">
<h1><a class="toc-backref" href="#id15">Proximal Policy Optimization</a><a class="headerlink" href="#proximal-policy-optimization" title="Permalink to this headline">¶</a></h1>
<div class="contents topic" id="table-of-contents">
<p class="topic-title first">Table of Contents</p>
<ul class="simple">
<li><a class="reference internal" href="#proximal-policy-optimization" id="id15">Proximal Policy Optimization</a><ul>
<li><a class="reference internal" href="#id1" id="id16">背景</a><ul>
<li><a class="reference internal" href="#id2" id="id17">速览</a></li>
<li><a class="reference internal" href="#id3" id="id18">关键方程</a></li>
<li><a class="reference internal" href="#id7" id="id19">探索与利用</a></li>
<li><a class="reference internal" href="#id8" id="id20">伪代码</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id9" id="id21">文档</a><ul>
<li><a class="reference internal" href="#id10" id="id22">保存的模型的内容</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id11" id="id23">参考</a><ul>
<li><a class="reference internal" href="#id12" id="id24">相关论文</a></li>
<li><a class="reference internal" href="#id13" id="id25">为什么是这些论文？</a></li>
<li><a class="reference internal" href="#id14" id="id26">其他公开实现</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="id1">
<h2><a class="toc-backref" href="#id16">背景</a><a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>(前一节： <a class="reference external" href="../algorithms/trpo.html#background">TRPO背景</a>)</p>
<p>PPO受到与TRPO相同的问题的激励：我们如何才能使用当前拥有的数据在策略上采取最大可能的改进步骤，
而又不会走得太远而导致意外使性能下降？在TRPO尝试使用复杂的二阶方法解决此问题的地方，
PPO是一阶方法的族，它们使用其他一些技巧来使新策略接近于旧策略。
PPO方法明显更易于实现，并且从经验上看，其性能至少与TRPO相同。</p>
<p>PPO有两种主要变体：PPO-Penalty和PPO-Clip。</p>
<p><strong>PPO-Penalty</strong> 近似解决了TRPO之类的受KL约束的更新，但是惩罚了目标函数中的KL背离而不是使其成为硬约束，
并且在训练过程中自动调整了惩罚系数，以便适当地缩放。</p>
<p><strong>PPO-Clip</strong> 在目标中没有KL散度项，也没有任何约束。取而代之的是依靠对目标函数的专门削减来消除新策略远离旧策略的动机。</p>
<p>在这里，我们仅关注PPO-Clip（OpenAI使用的主要变体）。</p>
<div class="section" id="id2">
<h3><a class="toc-backref" href="#id17">速览</a><a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li>PPO是在轨算法。</li>
<li>PPO可用于具有离散或连续动作空间的环境。</li>
<li>PPO的Spinning Up实现支持与MPI并行化。</li>
</ul>
</div>
<div class="section" id="id3">
<h3><a class="toc-backref" href="#id18">关键方程</a><a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>PPO-clip 通过以下更新策略</p>
<div class="math">
<p><span class="math">\theta_{k+1} = \arg \max_{\theta} \underset{s,a \sim \pi_{\theta_k}}{{\mathrm E}}\left[
    L(s,a,\theta_k, \theta)\right],</span></p>
</div><p>通常采取多个步骤（通常是小批量）SGD来最大化目标。这里 <span class="math">L</span> 是由</p>
<div class="math">
<p><span class="math">L(s,a,\theta_k,\theta) = \min\left(
\frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}  A^{\pi_{\theta_k}}(s,a), \;\;
\text{clip}\left(\frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}, 1 - \epsilon, 1+\epsilon \right) A^{\pi_{\theta_k}}(s,a)
\right),</span></p>
</div><p>其中 <span class="math">\epsilon</span> 是一个（小）超参数，它粗略地说出了新策略与旧策略的距离。</p>
<p>这是一个非常复杂的表述，很难一眼就知道它在做什么，或者它如何帮助使新策略接近旧策略。
事实证明，此目标有一个相当简化的版本 <a class="footnote-reference" href="#id5" id="id4">[1]</a>，它易于处理（也是我们在代码中实现的版本）：</p>
<div class="math">
<p><span class="math">L(s,a,\theta_k,\theta) = \min\left(
\frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}  A^{\pi_{\theta_k}}(s,a), \;\;
g(\epsilon, A^{\pi_{\theta_k}}(s,a))
\right),</span></p>
</div><p>其中</p>
<div class="math">
<p><span class="math">g(\epsilon, A) = \left\{
    \begin{array}{ll}
    (1 + \epsilon) A &amp; A \geq 0 \\
    (1 - \epsilon) A &amp; A &lt; 0.
    \end{array}
    \right.</span></p>
</div><p>为了弄清楚从中得到的直觉，让我们看一个状态对 <span class="math">(s,a)</span>，并分情况考虑。</p>
<p><strong>优势是正的</strong>：假设该状态-动作对的优势是正的，在这种情况下，它对目标的贡献减少为</p>
<div class="math">
<p><span class="math">L(s,a,\theta_k,\theta) = \min\left(
\frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}, (1 + \epsilon)
\right)  A^{\pi_{\theta_k}}(s,a).</span></p>
</div><p>因为优势是正的，所以如果采取行动的可能性更大，也就是说，如果 <span class="math">\pi_{\theta}(a|s)</span> 增加，则目标也会增加。
但是此术语中的最小值限制了目标可以增加的 <em>程度</em>。
一旦 <span class="math">\pi_{\theta}(a|s) &gt; (1+\epsilon) \pi_{\theta_k}(a|s)</span>，最小值就会增加，
此项达到 <span class="math">(1+\epsilon) A^{\pi_{\theta_k}}(s,a)</span> 的上限 。
因此：<em>远离旧策略不会使新政策受益</em>。</p>
<p><strong>优势是负的</strong>：假设该状态-动作对的优势是负的，在这种情况下，它对目标的贡献减少为</p>
<div class="math">
<p><span class="math">L(s,a,\theta_k,\theta) = \max\left(
\frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)}, (1 - \epsilon)
\right)  A^{\pi_{\theta_k}}(s,a).</span></p>
</div><p>因为优势是负的，所以如果行动变得不太可能（即 <span class="math">\pi_{\theta}(a|s)</span> 减小），则目标将增加。
但是此术语中的最大值限制了可以增加的 <em>程度</em>。
一旦 <span class="math">\pi_{\theta}(a|s) &lt; (1-\epsilon) \pi_{\theta_k}(a|s)</span>，最大值就会增加，
此项达到 <span class="math">(1-\epsilon) A^{\pi_{\theta_k}}(s,a)</span> 的上限。
因此，再次：<em>新政策不会因远离旧政策而受益</em>。</p>
<p>到目前为止，我们已经看到剪裁通过消除策略急剧变化的诱因而成为一种调节器，
而超参数 <span class="math">\epsilon</span> 对应于新策略与旧策略的距离的远近，同时仍然有利于实现目标。</p>
<div class="admonition- admonition">
<p class="first admonition-title">你应该知道</p>
<p>尽管这种削减对确保合理的策略更新大有帮助，但仍然有可能最终产生与旧策略相距太远的新策略，
并且不同的PPO实现使用很多技巧来避免这种情况。在此处的实现中，我们使用一种特别简单的方法：提前停止。
如果新策略与旧策略的平均KL散度差距超出阈值，我们将停止采取梯度步骤。</p>
<p class="last">如果你对基本的数学知识和实施细节感到良好，则有必要查看其他实施以了解它们如何处理此问题！</p>
</div>
<table class="docutils footnote" frame="void" id="id5" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id4">[1]</a></td><td>请参阅 <a class="reference external" href="https://drive.google.com/file/d/1PDzn9RPvaXjJFZkGeapMHbHGiWWW20Ey/view?usp=sharing">此说明</a>，以简化PPO-Clip目标的形式。</td></tr>
</tbody>
</table>
</div>
<div class="section" id="id7">
<h3><a class="toc-backref" href="#id19">探索与利用</a><a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>PPO以一种在轨策略方式训练随机策略。这意味着它会根据其随机策略的最新版本通过采样操作来进行探索。
动作选择的随机性取决于初始条件和训练程序。
在训练过程中，由于更新规则鼓励该策略利用已经发现的奖励，因此该策略通常变得越来越少随机性。
这可能会导致策略陷入局部最优状态。</p>
</div>
<div class="section" id="id8">
<h3><a class="toc-backref" href="#id20">伪代码</a><a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<div class="math">
<p><span class="math">\begin{algorithm}[H]
    \caption{PPO-Clip}
    \label{alg1}
\begin{algorithmic}[1]
    \STATE Input: initial policy parameters $\theta_0$, initial value function parameters $\phi_0$
    \FOR{$k = 0,1,2,...$}
    \STATE Collect set of trajectories ${\mathcal D}_k = \{\tau_i\}$ by running policy $\pi_k = \pi(\theta_k)$ in the environment.
    \STATE Compute rewards-to-go $\hat{R}_t$.
    \STATE Compute advantage estimates, $\hat{A}_t$ (using any method of advantage estimation) based on the current value function $V_{\phi_k}$.
    \STATE Update the policy by maximizing the PPO-Clip objective:
        \begin{equation*}
        \theta_{k+1} = \arg \max_{\theta} \frac{1}{|{\mathcal D}_k| T} \sum_{\tau \in {\mathcal D}_k} \sum_{t=0}^T \min\left(
            \frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_k}(a_t|s_t)}  A^{\pi_{\theta_k}}(s_t,a_t), \;\;
            g(\epsilon, A^{\pi_{\theta_k}}(s_t,a_t))
        \right),
        \end{equation*}
        typically via stochastic gradient ascent with Adam.
    \STATE Fit value function by regression on mean-squared error:
        \begin{equation*}
        \phi_{k+1} = \arg \min_{\phi} \frac{1}{|{\mathcal D}_k| T} \sum_{\tau \in {\mathcal D}_k} \sum_{t=0}^T\left( V_{\phi} (s_t) - \hat{R}_t \right)^2,
        \end{equation*}
        typically via some gradient descent algorithm.
    \ENDFOR
\end{algorithmic}
\end{algorithm}</span></p>
</div></div>
</div>
<div class="section" id="id9">
<h2><a class="toc-backref" href="#id21">文档</a><a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="spinup.ppo">
<code class="descclassname">spinup.</code><code class="descname">ppo</code><span class="sig-paren">(</span><em>env_fn</em>, <em>actor_critic=&lt;function mlp_actor_critic&gt;</em>, <em>ac_kwargs={}</em>, <em>seed=0</em>, <em>steps_per_epoch=4000</em>, <em>epochs=50</em>, <em>gamma=0.99</em>, <em>clip_ratio=0.2</em>, <em>pi_lr=0.0003</em>, <em>vf_lr=0.001</em>, <em>train_pi_iters=80</em>, <em>train_v_iters=80</em>, <em>lam=0.97</em>, <em>max_ep_len=1000</em>, <em>target_kl=0.01</em>, <em>logger_kwargs={}</em>, <em>save_freq=10</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/spinup/algos/ppo/ppo.html#ppo"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#spinup.ppo" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>env_fn</strong> &#8211; A function which creates a copy of the environment.
The environment must satisfy the OpenAI Gym API.</li>
<li><strong>actor_critic</strong> &#8211; <p>A function which takes in placeholder symbols
for state, <code class="docutils literal"><span class="pre">x_ph</span></code>, and action, <code class="docutils literal"><span class="pre">a_ph</span></code>, and returns the main
outputs from the agent&#8217;s Tensorflow computation graph:</p>
<table border="1" class="docutils">
<colgroup>
<col width="16%" />
<col width="24%" />
<col width="60%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Symbol</th>
<th class="head">Shape</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">pi</span></code></td>
<td>(batch, act_dim)</td>
<td><div class="first last line-block">
<div class="line">Samples actions from policy given</div>
<div class="line">states.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">logp</span></code></td>
<td>(batch,)</td>
<td><div class="first last line-block">
<div class="line">Gives log probability, according to</div>
<div class="line">the policy, of taking actions <code class="docutils literal"><span class="pre">a_ph</span></code></div>
<div class="line">in states <code class="docutils literal"><span class="pre">x_ph</span></code>.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">logp_pi</span></code></td>
<td>(batch,)</td>
<td><div class="first last line-block">
<div class="line">Gives log probability, according to</div>
<div class="line">the policy, of the action sampled by</div>
<div class="line"><code class="docutils literal"><span class="pre">pi</span></code>.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">v</span></code></td>
<td>(batch,)</td>
<td><div class="first last line-block">
<div class="line">Gives the value estimate for states</div>
<div class="line">in <code class="docutils literal"><span class="pre">x_ph</span></code>. (Critical: make sure</div>
<div class="line">to flatten this!)</div>
</div>
</td>
</tr>
</tbody>
</table>
</li>
<li><strong>ac_kwargs</strong> (<em>dict</em>) &#8211; Any kwargs appropriate for the actor_critic
function you provided to PPO.</li>
<li><strong>seed</strong> (<em>int</em>) &#8211; Seed for random number generators.</li>
<li><strong>steps_per_epoch</strong> (<em>int</em>) &#8211; Number of steps of interaction (state-action pairs)
for the agent and the environment in each epoch.</li>
<li><strong>epochs</strong> (<em>int</em>) &#8211; Number of epochs of interaction (equivalent to
number of policy updates) to perform.</li>
<li><strong>gamma</strong> (<em>float</em>) &#8211; Discount factor. (Always between 0 and 1.)</li>
<li><strong>clip_ratio</strong> (<em>float</em>) &#8211; Hyperparameter for clipping in the policy objective.
Roughly: how far can the new policy go from the old policy while
still profiting (improving the objective function)? The new policy
can still go farther than the clip_ratio says, but it doesn&#8217;t help
on the objective anymore. (Usually small, 0.1 to 0.3.)</li>
<li><strong>pi_lr</strong> (<em>float</em>) &#8211; Learning rate for policy optimizer.</li>
<li><strong>vf_lr</strong> (<em>float</em>) &#8211; Learning rate for value function optimizer.</li>
<li><strong>train_pi_iters</strong> (<em>int</em>) &#8211; Maximum number of gradient descent steps to take
on policy loss per epoch. (Early stopping may cause optimizer
to take fewer than this.)</li>
<li><strong>train_v_iters</strong> (<em>int</em>) &#8211; Number of gradient descent steps to take on
value function per epoch.</li>
<li><strong>lam</strong> (<em>float</em>) &#8211; Lambda for GAE-Lambda. (Always between 0 and 1,
close to 1.)</li>
<li><strong>max_ep_len</strong> (<em>int</em>) &#8211; Maximum length of trajectory / episode / rollout.</li>
<li><strong>target_kl</strong> (<em>float</em>) &#8211; Roughly what KL divergence we think is appropriate
between new and old policies after an update. This will get used
for early stopping. (Usually small, 0.01 or 0.05.)</li>
<li><strong>logger_kwargs</strong> (<em>dict</em>) &#8211; Keyword args for EpochLogger.</li>
<li><strong>save_freq</strong> (<em>int</em>) &#8211; How often (in terms of gap between epochs) to save
the current policy and value function.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<div class="section" id="id10">
<h3><a class="toc-backref" href="#id22">保存的模型的内容</a><a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<p>记录的计算图包括：</p>
<table border="1" class="docutils">
<colgroup>
<col width="11%" />
<col width="89%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">键</th>
<th class="head">值</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">x</span></code></td>
<td>Tensorflow placeholder for state input.</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">pi</span></code></td>
<td>Samples an action from the agent, conditioned on states in <code class="docutils literal"><span class="pre">x</span></code>.</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">v</span></code></td>
<td>Gives value estimate for states in <code class="docutils literal"><span class="pre">x</span></code>.</td>
</tr>
</tbody>
</table>
<p>可以通过以下方式访问此保存的模型</p>
<ul class="simple">
<li>使用 <a class="reference external" href="../user/saving_and_loading.html#loading-and-running-trained-policies">test_policy.py</a> 工具运行经过训练的策略，</li>
<li>或使用 <a class="reference external" href="../utils/logger.html#spinup.utils.logx.restore_tf_graph">restore_tf_graph</a> 将整个保存的图形加载到程序中。</li>
</ul>
</div>
</div>
<div class="section" id="id11">
<h2><a class="toc-backref" href="#id23">参考</a><a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id12">
<h3><a class="toc-backref" href="#id24">相关论文</a><a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="https://arxiv.org/abs/1707.06347">Proximal Policy Optimization Algorithms</a>, Schulman et al. 2017</li>
<li><a class="reference external" href="https://arxiv.org/abs/1506.02438">High Dimensional Continuous Control Using Generalized Advantage Estimation</a>, Schulman et al. 2016</li>
<li><a class="reference external" href="https://arxiv.org/abs/1707.02286">Emergence of Locomotion Behaviours in Rich Environments</a>, Heess et al. 2017</li>
</ul>
</div>
<div class="section" id="id13">
<h3><a class="toc-backref" href="#id25">为什么是这些论文？</a><a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h3>
<p>包含Schulman 2017是因为它是描述PPO的原始论文。
之所以包含Schulman 2016，是因为我们对PPO的实现利用了通用优势估计来计算策略梯度。
包含了Heess 2017，因为它提供了对复杂环境中PPO代理所学行为的大规模实证分析（尽管它使用PPO-Penalty而不是PPO-clip）。</p>
</div>
<div class="section" id="id14">
<h3><a class="toc-backref" href="#id26">其他公开实现</a><a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="https://github.com/openai/baselines/tree/master/baselines/ppo2">Baselines</a></li>
<li><a class="reference external" href="https://github.com/joschu/modular_rl/blob/master/modular_rl/ppo.py">ModularRL</a> （注意：这个实现了PPO-penalty而不是PPO-clip。）</li>
<li><a class="reference external" href="https://github.com/rll/rllab/blob/master/rllab/algos/ppo.py">rllab</a> （注意：这个实现了PPO-penalty而不是PPO-clip。）</li>
<li><a class="reference external" href="https://github.com/ray-project/ray/tree/master/python/ray/rllib/agents/ppo">rllib (Ray)</a></li>
</ul>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="ddpg.html" class="btn btn-neutral float-right" title="Deep Deterministic Policy Gradient" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="trpo.html" class="btn btn-neutral" title="Trust Region Policy Optimization" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, OpenAI.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-46660488-7"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-46660488-7');
</script>



</body>
</html>