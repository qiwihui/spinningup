

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Trust Region Policy Optimization &mdash; Spinning Up  documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/openai_icon.ico"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/modify.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Proximal Policy Optimization" href="ppo.html" />
    <link rel="prev" title="Vanilla Policy Gradient" href="vpg.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/spinning-up-logo2.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          

            
            
              
            
            
              <p class="caption"><span class="caption-text">用户文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../user/introduction.html">介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/algorithms.html">算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/running.html">运行试验</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/saving_and_loading.html">试验输出</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/plotting.html">绘制结果</a></li>
</ul>
<p class="caption"><span class="caption-text">强化学习介绍</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/rl_intro.html">第一部分：强化学习中的核心概念</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/rl_intro2.html">第二部分：强化学习算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/rl_intro3.html">第三部分：策略优化介绍</a></li>
</ul>
<p class="caption"><span class="caption-text">资源</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/spinningup.html">深度强化学习研究者资料</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/keypapers.html">深度强化学习的核心论文</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/exercises.html">练习</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/bench.html">Spinning Up 算法实现的基准</a></li>
</ul>
<p class="caption"><span class="caption-text">算法文档</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="vpg.html">Vanilla Policy Gradient</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Trust Region Policy Optimization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#background">背景</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id3">速览</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id4">关键方程</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id10">探索与利用</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id11">伪代码</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id12">文档</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id13">保存的模型的内容</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id14">参考</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id15">相关论文</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id17">为什么是这些论文？</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id18">其他公开实现</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ppo.html">Proximal Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddpg.html">Deep Deterministic Policy Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="td3.html">Twin Delayed DDPG</a></li>
<li class="toctree-l1"><a class="reference internal" href="sac.html">Soft Actor-Critic</a></li>
</ul>
<p class="caption"><span class="caption-text">工具文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../utils/logger.html">日志记录</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/plotter.html">绘图</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/mpi.html">MPI工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/run_utils.html">运行工具</a></li>
</ul>
<p class="caption"><span class="caption-text">其他</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../etc/acknowledgements.html">致谢</a></li>
<li class="toctree-l1"><a class="reference internal" href="../etc/author.html">关于作者</a></li>
</ul>

            
          
<style>
.adsbygoogle-style {
  background-color: #404452;
  margin-top: 20px;
}
</style>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- square-ad -->
<ins class="adsbygoogle adsbygoogle-style"
     style="display:block"
     data-ad-client="ca-pub-8935595858652656"
     data-ad-slot="2636787302"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Spinning Up</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Trust Region Policy Optimization</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/algorithms/trpo.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="trust-region-policy-optimization">
<h1><a class="toc-backref" href="#id19">Trust Region Policy Optimization</a><a class="headerlink" href="#trust-region-policy-optimization" title="Permalink to this headline">¶</a></h1>
<div class="contents topic" id="id1">
<p class="topic-title first">目录</p>
<ul class="simple">
<li><a class="reference internal" href="#trust-region-policy-optimization" id="id19">Trust Region Policy Optimization</a><ul>
<li><a class="reference internal" href="#background" id="id20">背景</a><ul>
<li><a class="reference internal" href="#id3" id="id21">速览</a></li>
<li><a class="reference internal" href="#id4" id="id22">关键方程</a></li>
<li><a class="reference internal" href="#id10" id="id23">探索与利用</a></li>
<li><a class="reference internal" href="#id11" id="id24">伪代码</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id12" id="id25">文档</a><ul>
<li><a class="reference internal" href="#id13" id="id26">保存的模型的内容</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id14" id="id27">参考</a><ul>
<li><a class="reference internal" href="#id15" id="id28">相关论文</a></li>
<li><a class="reference internal" href="#id17" id="id29">为什么是这些论文？</a></li>
<li><a class="reference internal" href="#id18" id="id30">其他公开实现</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="background">
<span id="id2"></span><h2><a class="toc-backref" href="#id20">背景</a><a class="headerlink" href="#background" title="Permalink to this headline">¶</a></h2>
<p>（前一节： <a class="reference external" href="../algorithms/vpg.html#background">VPG背景</a>）</p>
<p>TRPO通过采取最大的可以改进策略的步骤来更新策略，同时满足关于允许新旧策略接近的特殊约束。
约束用 <a class="reference external" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL散度</a> 表示，KL散度是对概率分布之间的距离（但不完全相同）的一种度量。</p>
<p>这与常规策略梯度不同，后者使新策略和旧策略在参数空间中保持紧密联系。
但是，即使参数空间上看似很小的差异也可能在性能上产生很大的差异──因此，一个糟糕的步骤可能会使策略性能崩溃。
这使得使用大步长的vanilla policy gradients变得危险，从而损害了其采样效率。
TRPO很好地避免了这种崩溃，并且倾向于快速单调地提高性能。</p>
<div class="section" id="id3">
<h3><a class="toc-backref" href="#id21">速览</a><a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li>TRPO是在轨算法。</li>
<li>TRPO可用于具有离散或连续动作空间的环境。</li>
<li>TRPO的Spinning Up实现支持与MPI并行化。</li>
</ul>
</div>
<div class="section" id="id4">
<h3><a class="toc-backref" href="#id22">关键方程</a><a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>令 <span class="math">\pi_{\theta}</span> 表示参数为 <span class="math">\theta</span> 的策略，理论上的TRPO更新为：</p>
<div class="math">
<p><span class="math">\theta_{k+1} = \arg \max_{\theta} \; &amp; {\mathcal L}(\theta_k, \theta) \\
\text{s.t.} \; &amp; \bar{D}_{KL}(\theta || \theta_k) \leq \delta</span></p>
</div><p>其中 <span class="math">{\mathcal L}(\theta_k, \theta)</span> 是 <em>替代优势</em>，
它使用旧策略中的数据来衡量策略 <span class="math">\pi_{\theta}</span> 与旧策略 <span class="math">\pi_{\theta_k}</span> 的相对性能：</p>
<div class="math">
<p><span class="math">{\mathcal L}(\theta_k, \theta) = \underE{s,a \sim \pi_{\theta_k}}{
    \frac{\pi_{\theta}(a|s)}{\pi_{\theta_k}(a|s)} A^{\pi_{\theta_k}}(s,a)
    },</span></p>
</div><p><span class="math">\bar{D}_{KL}(\theta || \theta_k)</span> 是旧策略访问的各状态之间的策略之间的平均散度差异：</p>
<div class="math">
<p><span class="math">\bar{D}_{KL}(\theta || \theta_k) = \underE{s \sim \pi_{\theta_k}}{
    D_{KL}\left(\pi_{\theta}(\cdot|s) || \pi_{\theta_k} (\cdot|s) \right)
}.</span></p>
</div><div class="admonition- admonition">
<p class="first admonition-title">你应该知道</p>
<p class="last">当 <span class="math">\theta = \theta_k</span> 时，目标和约束都为零。
此外，当 <span class="math">\theta = \theta_k</span> 时，约束相对于 <span class="math">\theta</span> 的梯度为零。
要证明这些事实，需要对相关数学有一些微妙的掌握──每当您准备就绪时，这都是值得做的练习！</p>
</div>
<p>理论上的TRPO更新不是最容易使用的，因此TRPO做出了一些近似以快速获得答案。
我们使用泰勒展开将目标和约束扩展到 <span class="math">\theta_k</span> 周围的首阶指数（leading order）：</p>
<div class="math">
<p><span class="math">{\mathcal L}(\theta_k, \theta) &amp;\approx g^T (\theta - \theta_k) \\
\bar{D}_{KL}(\theta || \theta_k) &amp; \approx \frac{1}{2} (\theta - \theta_k)^T H (\theta - \theta_k)</span></p>
</div><p>结果产生一个近似的优化问题，</p>
<div class="math">
<p><span class="math">\theta_{k+1} = \arg \max_{\theta} \; &amp; g^T (\theta - \theta_k) \\
\text{s.t.} \; &amp; \frac{1}{2} (\theta - \theta_k)^T H (\theta - \theta_k) \leq \delta.</span></p>
</div><div class="admonition- admonition">
<p class="first admonition-title">你应该知道</p>
<p class="last">巧合的是，以 <span class="math">\theta = \theta_k</span> 评估的替代优势函数相对于 <span class="math">\theta</span> 的梯度 <span class="math">g</span>
恰好等于策略梯度 <span class="math">\nabla_{\theta} J(\pi_{\theta})</span>！
如果您愿意精通数学，请尝试证明这一点。</p>
</div>
<p>这个近似问题可以通过拉格朗日对偶 <a class="footnote-reference" href="#id6" id="id5">[1]</a> 的方法来解析地解决，得出的结果是：</p>
<div class="math">
<p><span class="math">\theta_{k+1} = \theta_k + \sqrt{\frac{2 \delta}{g^T H^{-1} g}} H^{-1} g.</span></p>
</div><p>如果我们到此为止，并仅使用此最终结果，该算法将准确地计算 <a class="reference external" href="https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf">自然策略梯度</a> （Natural Policy Gradient）。
一个问题是，由于泰勒展开式引入的近似误差，这可能无法满足KL约束，或实际上提高了替代优势。
TRPO对此更新规则进行了修改：回溯行搜索，</p>
<div class="math">
<p><span class="math">\theta_{k+1} = \theta_k + \alpha^j \sqrt{\frac{2 \delta}{g^T H^{-1} g}} H^{-1} g,</span></p>
</div><p>其中 <span class="math">\alpha \in (0,1)</span> 是回溯系数，
<span class="math">j</span> 是 <span class="math">\pi_{\theta_{k+1}}</span> 满足KL约束并产生正的替代优势的最小非负整数。</p>
<p>Lastly: computing and storing the matrix inverse, <span class="math">H^{-1}</span>, is painfully expensive when dealing with neural network policies with thousands or millions of parameters.
TRPO sidesteps the issue by using the <a href="#id31"><span class="problematic" id="id32">`conjugate gradient`_</span></a> algorithm to solve <span class="math">Hx = g</span> for <span class="math">x = H^{-1} g</span>,
requiring only a function which can compute the matrix-vector product <span class="math">Hx</span> instead of computing and storing the whole matrix <span class="math">H</span> directly.
This is not too hard to do: we set up a symbolic operation to calculate</p>
<p>最后：处理带有成千上万个参数的神经网络策略时，矩阵逆 <span class="math">H^{-1}</span> 的计算和存储非常昂贵。
TRPO通过使用 <a class="reference external" href="https://en.wikipedia.org/wiki/Conjugate_gradient_method">共轭梯度</a> 算法对 <span class="math">x = H^{-1} g</span> 求解 <span class="math">Hx = g</span> 来回避问题，
仅需要一个可以计算矩阵矢量乘积 <span class="math">Hx</span> 的函数，而不是直接计算和存储整个矩阵 <span class="math">H</span>。
这并不难：我们设置了一个符号运算来计算</p>
<div class="math">
<p><span class="math">Hx = \nabla_{\theta} \left( \left(\nabla_{\theta} \bar{D}_{KL}(\theta || \theta_k)\right)^T x \right),</span></p>
</div><p>这样就可以在不计算整个矩阵的情况下提供正确的输出。</p>
<table class="docutils footnote" frame="void" id="id6" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id5">[1]</a></td><td>参见Boyd和Vandenberghe的 <a class="reference external" href="http://stanford.edu/~boyd/cvxbook/">凸优化</a>，特别是第2至第5章。</td></tr>
</tbody>
</table>
</div>
<div class="section" id="id10">
<h3><a class="toc-backref" href="#id23">探索与利用</a><a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<p>TRPO trains a stochastic policy in an on-policy way. This means that it explores by sampling actions according to the latest version of its stochastic policy. The amount of randomness in action selection depends on both initial conditions and the training procedure. Over the course of training, the policy typically becomes progressively less random, as the update rule encourages it to exploit rewards that it has already found. This may cause the policy to get trapped in local optima.</p>
<p>TRPO以一种在轨策略方式训练随机策略。这意味着它会根据其随机策略的最新版本通过采样操作来进行探索。
动作选择的随机性取决于初始条件和训练程序。
在训练过程中，由于更新规则鼓励该策略利用已经发现的奖励，因此该策略通常变得越来越少随机性。
这可能会导致策略陷入局部最优状态。</p>
</div>
<div class="section" id="id11">
<h3><a class="toc-backref" href="#id24">伪代码</a><a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h3>
<div class="math">
<p><span class="math">\begin{algorithm}[H]
    \caption{Trust Region Policy Optimization}
    \label{alg1}
\begin{algorithmic}[1]
    \STATE Input: initial policy parameters $\theta_0$, initial value function parameters $\phi_0$
    \STATE Hyperparameters: KL-divergence limit $\delta$, backtracking coefficient $\alpha$, maximum number of backtracking steps $K$
    \FOR{$k = 0,1,2,...$}
    \STATE Collect set of trajectories ${\mathcal D}_k = \{\tau_i\}$ by running policy $\pi_k = \pi(\theta_k)$ in the environment.
    \STATE Compute rewards-to-go $\hat{R}_t$.
    \STATE Compute advantage estimates, $\hat{A}_t$ (using any method of advantage estimation) based on the current value function $V_{\phi_k}$.
    \STATE Estimate policy gradient as
        \begin{equation*}
        \hat{g}_k = \frac{1}{|{\mathcal D}_k|} \sum_{\tau \in {\mathcal D}_k} \sum_{t=0}^T \left. \nabla_{\theta} \log\pi_{\theta}(a_t|s_t)\right|_{\theta_k} \hat{A}_t.
        \end{equation*}
    \STATE Use the conjugate gradient algorithm to compute
        \begin{equation*}
        \hat{x}_k \approx \hat{H}_k^{-1} \hat{g}_k,
        \end{equation*}
        where $\hat{H}_k$ is the Hessian of the sample average KL-divergence.
    \STATE Update the policy by backtracking line search with
        \begin{equation*}
        \theta_{k+1} = \theta_k + \alpha^j \sqrt{ \frac{2\delta}{\hat{x}_k^T \hat{H}_k \hat{x}_k}} \hat{x}_k,
        \end{equation*}
        where $j \in \{0, 1, 2, ... K\}$ is the smallest value which improves the sample loss and satisfies the sample KL-divergence constraint.
    \STATE Fit value function by regression on mean-squared error:
        \begin{equation*}
        \phi_{k+1} = \arg \min_{\phi} \frac{1}{|{\mathcal D}_k| T} \sum_{\tau \in {\mathcal D}_k} \sum_{t=0}^T\left( V_{\phi} (s_t) - \hat{R}_t \right)^2,
        \end{equation*}
        typically via some gradient descent algorithm.
    \ENDFOR
\end{algorithmic}
\end{algorithm}</span></p>
</div></div>
</div>
<div class="section" id="id12">
<h2><a class="toc-backref" href="#id25">文档</a><a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="spinup.trpo">
<code class="descclassname">spinup.</code><code class="descname">trpo</code><span class="sig-paren">(</span><em>env_fn</em>, <em>actor_critic=&lt;function mlp_actor_critic&gt;</em>, <em>ac_kwargs={}</em>, <em>seed=0</em>, <em>steps_per_epoch=4000</em>, <em>epochs=50</em>, <em>gamma=0.99</em>, <em>delta=0.01</em>, <em>vf_lr=0.001</em>, <em>train_v_iters=80</em>, <em>damping_coeff=0.1</em>, <em>cg_iters=10</em>, <em>backtrack_iters=10</em>, <em>backtrack_coeff=0.8</em>, <em>lam=0.97</em>, <em>max_ep_len=1000</em>, <em>logger_kwargs={}</em>, <em>save_freq=10</em>, <em>algo='trpo'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/spinup/algos/trpo/trpo.html#trpo"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#spinup.trpo" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>env_fn</strong> &#8211; A function which creates a copy of the environment.
The environment must satisfy the OpenAI Gym API.</li>
<li><strong>actor_critic</strong> &#8211; <p>A function which takes in placeholder symbols
for state, <code class="docutils literal"><span class="pre">x_ph</span></code>, and action, <code class="docutils literal"><span class="pre">a_ph</span></code>, and returns the main
outputs from the agent&#8217;s Tensorflow computation graph:</p>
<table border="1" class="docutils">
<colgroup>
<col width="18%" />
<col width="24%" />
<col width="59%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Symbol</th>
<th class="head">Shape</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">pi</span></code></td>
<td>(batch, act_dim)</td>
<td><div class="first last line-block">
<div class="line">Samples actions from policy given</div>
<div class="line">states.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">logp</span></code></td>
<td>(batch,)</td>
<td><div class="first last line-block">
<div class="line">Gives log probability, according to</div>
<div class="line">the policy, of taking actions <code class="docutils literal"><span class="pre">a_ph</span></code></div>
<div class="line">in states <code class="docutils literal"><span class="pre">x_ph</span></code>.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">logp_pi</span></code></td>
<td>(batch,)</td>
<td><div class="first last line-block">
<div class="line">Gives log probability, according to</div>
<div class="line">the policy, of the action sampled by</div>
<div class="line"><code class="docutils literal"><span class="pre">pi</span></code>.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">info</span></code></td>
<td>N/A</td>
<td><div class="first last line-block">
<div class="line">A dict of any intermediate quantities</div>
<div class="line">(from calculating the policy or log</div>
<div class="line">probabilities) which are needed for</div>
<div class="line">analytically computing KL divergence.</div>
<div class="line">(eg sufficient statistics of the</div>
<div class="line">distributions)</div>
</div>
</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">info_phs</span></code></td>
<td>N/A</td>
<td><div class="first last line-block">
<div class="line">A dict of placeholders for old values</div>
<div class="line">of the entries in <code class="docutils literal"><span class="pre">info</span></code>.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">d_kl</span></code></td>
<td>()</td>
<td><div class="first last line-block">
<div class="line">A symbol for computing the mean KL</div>
<div class="line">divergence between the current policy</div>
<div class="line">(<code class="docutils literal"><span class="pre">pi</span></code>) and the old policy (as</div>
<div class="line">specified by the inputs to</div>
<div class="line"><code class="docutils literal"><span class="pre">info_phs</span></code>) over the batch of</div>
<div class="line">states given in <code class="docutils literal"><span class="pre">x_ph</span></code>.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">v</span></code></td>
<td>(batch,)</td>
<td><div class="first last line-block">
<div class="line">Gives the value estimate for states</div>
<div class="line">in <code class="docutils literal"><span class="pre">x_ph</span></code>. (Critical: make sure</div>
<div class="line">to flatten this!)</div>
</div>
</td>
</tr>
</tbody>
</table>
</li>
<li><strong>ac_kwargs</strong> (<em>dict</em>) &#8211; Any kwargs appropriate for the actor_critic
function you provided to TRPO.</li>
<li><strong>seed</strong> (<em>int</em>) &#8211; Seed for random number generators.</li>
<li><strong>steps_per_epoch</strong> (<em>int</em>) &#8211; Number of steps of interaction (state-action pairs)
for the agent and the environment in each epoch.</li>
<li><strong>epochs</strong> (<em>int</em>) &#8211; Number of epochs of interaction (equivalent to
number of policy updates) to perform.</li>
<li><strong>gamma</strong> (<em>float</em>) &#8211; Discount factor. (Always between 0 and 1.)</li>
<li><strong>delta</strong> (<em>float</em>) &#8211; KL-divergence limit for TRPO / NPG update.
(Should be small for stability. Values like 0.01, 0.05.)</li>
<li><strong>vf_lr</strong> (<em>float</em>) &#8211; Learning rate for value function optimizer.</li>
<li><strong>train_v_iters</strong> (<em>int</em>) &#8211; Number of gradient descent steps to take on
value function per epoch.</li>
<li><strong>damping_coeff</strong> (<em>float</em>) &#8211; <p>Artifact for numerical stability, should be
smallish. Adjusts Hessian-vector product calculation:</p>
<div class="math">
<p><span class="math">Hv \rightarrow (\alpha I + H)v</span></p>
</div><p>where <span class="math">\alpha</span> is the damping coefficient.
Probably don&#8217;t play with this hyperparameter.</p>
</li>
<li><strong>cg_iters</strong> (<em>int</em>) &#8211; <p>Number of iterations of conjugate gradient to perform.
Increasing this will lead to a more accurate approximation
to <span class="math">H^{-1} g</span>, and possibly slightly-improved performance,
but at the cost of slowing things down.</p>
<p>Also probably don&#8217;t play with this hyperparameter.</p>
</li>
<li><strong>backtrack_iters</strong> (<em>int</em>) &#8211; Maximum number of steps allowed in the
backtracking line search. Since the line search usually doesn&#8217;t
backtrack, and usually only steps back once when it does, this
hyperparameter doesn&#8217;t often matter.</li>
<li><strong>backtrack_coeff</strong> (<em>float</em>) &#8211; How far back to step during backtracking line
search. (Always between 0 and 1, usually above 0.5.)</li>
<li><strong>lam</strong> (<em>float</em>) &#8211; Lambda for GAE-Lambda. (Always between 0 and 1,
close to 1.)</li>
<li><strong>max_ep_len</strong> (<em>int</em>) &#8211; Maximum length of trajectory / episode / rollout.</li>
<li><strong>logger_kwargs</strong> (<em>dict</em>) &#8211; Keyword args for EpochLogger.</li>
<li><strong>save_freq</strong> (<em>int</em>) &#8211; How often (in terms of gap between epochs) to save
the current policy and value function.</li>
<li><strong>algo</strong> &#8211; Either &#8216;trpo&#8217; or &#8216;npg&#8217;: this code supports both, since they are
almost the same.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<div class="section" id="id13">
<h3><a class="toc-backref" href="#id26">保存的模型的内容</a><a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h3>
<p>记录的计算图包括：</p>
<table border="1" class="docutils">
<colgroup>
<col width="11%" />
<col width="89%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">键</th>
<th class="head">值</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">x</span></code></td>
<td>Tensorflow placeholder for state input.</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">pi</span></code></td>
<td>Samples an action from the agent, conditioned on states in <code class="docutils literal"><span class="pre">x</span></code>.</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">v</span></code></td>
<td>Gives value estimate for states in <code class="docutils literal"><span class="pre">x</span></code>.</td>
</tr>
</tbody>
</table>
<p>可以通过以下方式访问此保存的模型</p>
<ul class="simple">
<li>使用 <a class="reference external" href="../user/saving_and_loading.html#loading-and-running-trained-policies">test_policy.py</a> 工具运行经过训练的策略，</li>
<li>或使用 <a class="reference external" href="../utils/logger.html#spinup.utils.logx.restore_tf_graph">restore_tf_graph</a> 将整个保存的图形加载到程序中。</li>
</ul>
</div>
</div>
<div class="section" id="id14">
<h2><a class="toc-backref" href="#id27">参考</a><a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id15">
<h3><a class="toc-backref" href="#id28">相关论文</a><a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="https://arxiv.org/abs/1502.05477">Trust Region Policy Optimization</a>, Schulman et al. 2015</li>
<li><a class="reference external" href="https://arxiv.org/abs/1506.02438">High Dimensional Continuous Control Using Generalized Advantage Estimation</a>, Schulman et al. 2016</li>
<li><a class="reference external" href="https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf">Approximately Optimal Approximate Reinforcement Learning</a>, Kakade and Langford 2002</li>
</ul>
</div>
<div class="section" id="id17">
<h3><a class="toc-backref" href="#id29">为什么是这些论文？</a><a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h3>
<p>包含Schulman 2015是因为它是描述TRPO的原始论文。
之所以包含Schulman 2016，是因为我们对TRPO的实现利用了通用优势估计来计算策略梯度。
之所以将Kakade和Langford 2002包括在内是因为它包含的理论结果激励并深深地与TRPO的理论基础联系在一起。</p>
</div>
<div class="section" id="id18">
<h3><a class="toc-backref" href="#id30">其他公开实现</a><a class="headerlink" href="#id18" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="https://github.com/openai/baselines/tree/master/baselines/trpo_mpi">Baselines</a></li>
<li><a class="reference external" href="https://github.com/joschu/modular_rl/blob/master/modular_rl/trpo.py">ModularRL</a></li>
<li><a class="reference external" href="https://github.com/rll/rllab/blob/master/rllab/algos/trpo.py">rllab</a></li>
</ul>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="ppo.html" class="btn btn-neutral float-right" title="Proximal Policy Optimization" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="vpg.html" class="btn btn-neutral" title="Vanilla Policy Gradient" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, OpenAI.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-46660488-7"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-46660488-7');
</script>



</body>
</html>