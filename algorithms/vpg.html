

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Vanilla Policy Gradient &mdash; Spinning Up  documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/openai_icon.ico"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/modify.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Trust Region Policy Optimization" href="trpo.html" />
    <link rel="prev" title="Spinning Up 算法实现的基准" href="../spinningup/bench.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/spinning-up-logo2.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          

            
            
              
            
            
              <p class="caption"><span class="caption-text">用户文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../user/introduction.html">介绍</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/installation.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/algorithms.html">算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/running.html">运行试验</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/saving_and_loading.html">试验输出</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user/plotting.html">绘制结果</a></li>
</ul>
<p class="caption"><span class="caption-text">强化学习介绍</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/rl_intro.html">第一部分：强化学习中的核心概念</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/rl_intro2.html">第二部分：强化学习算法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/rl_intro3.html">第三部分：策略优化介绍</a></li>
</ul>
<p class="caption"><span class="caption-text">资源</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/spinningup.html">深度强化学习研究者资料</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/keypapers.html">深度强化学习的核心论文</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/exercises.html">练习</a></li>
<li class="toctree-l1"><a class="reference internal" href="../spinningup/bench.html">Spinning Up 算法实现的基准</a></li>
</ul>
<p class="caption"><span class="caption-text">算法文档</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Vanilla Policy Gradient</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#background">背景</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id4">速览</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id5">关键方程</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id6">探索与利用</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id7">伪代码</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id8">文档</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id9">保存的模型的内容</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id10">参考</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id11">相关论文</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id12">为什么是这些论文？</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id13">其他公开实现</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="trpo.html">Trust Region Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="ppo.html">Proximal Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="ddpg.html">Deep Deterministic Policy Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="td3.html">Twin Delayed DDPG</a></li>
<li class="toctree-l1"><a class="reference internal" href="sac.html">Soft Actor-Critic</a></li>
</ul>
<p class="caption"><span class="caption-text">工具文档</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../utils/logger.html">日志记录</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/plotter.html">绘图</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/mpi.html">MPI工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../utils/run_utils.html">运行工具</a></li>
</ul>
<p class="caption"><span class="caption-text">其他</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../etc/acknowledgements.html">致谢</a></li>
<li class="toctree-l1"><a class="reference internal" href="../etc/author.html">关于作者</a></li>
</ul>

            
          
<style>
.adsbygoogle-style {
  background-color: #404452;
  margin-top: 20px;
}
</style>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- square-ad -->
<ins class="adsbygoogle adsbygoogle-style"
     style="display:block"
     data-ad-client="ca-pub-8935595858652656"
     data-ad-slot="2636787302"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Spinning Up</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Vanilla Policy Gradient</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/algorithms/vpg.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="vanilla-policy-gradient">
<h1><a class="toc-backref" href="#id14">Vanilla Policy Gradient</a><a class="headerlink" href="#vanilla-policy-gradient" title="Permalink to this headline">¶</a></h1>
<div class="contents topic" id="id1">
<p class="topic-title first">目录</p>
<ul class="simple">
<li><a class="reference internal" href="#vanilla-policy-gradient" id="id14">Vanilla Policy Gradient</a><ul>
<li><a class="reference internal" href="#background" id="id15">背景</a><ul>
<li><a class="reference internal" href="#id4" id="id16">速览</a></li>
<li><a class="reference internal" href="#id5" id="id17">关键方程</a></li>
<li><a class="reference internal" href="#id6" id="id18">探索与利用</a></li>
<li><a class="reference internal" href="#id7" id="id19">伪代码</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id8" id="id20">文档</a><ul>
<li><a class="reference internal" href="#id9" id="id21">保存的模型的内容</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id10" id="id22">参考</a><ul>
<li><a class="reference internal" href="#id11" id="id23">相关论文</a></li>
<li><a class="reference internal" href="#id12" id="id24">为什么是这些论文？</a></li>
<li><a class="reference internal" href="#id13" id="id25">其他公开实现</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="background">
<span id="id2"></span><h2><a class="toc-backref" href="#id15">背景</a><a class="headerlink" href="#background" title="Permalink to this headline">¶</a></h2>
<p>（前一节：<a class="reference external" href="../spinningup/rl_intro3.html">强化学习介绍：第三部分</a>）</p>
<p>策略梯度背后的关键思想是提高导致更高回报的动作的概率，并降低导致更低回报的动作的概率，直到你获得最佳策略。</p>
<div class="section" id="id4">
<h3><a class="toc-backref" href="#id16">速览</a><a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li>VPG 是一种在轨策略算法。</li>
<li>VPG可用于具有离散或连续动作空间的环境。</li>
<li>VPG的Spinning Up实现支持与MPI并行化。</li>
</ul>
</div>
<div class="section" id="id5">
<h3><a class="toc-backref" href="#id17">关键方程</a><a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>令 <span class="math">\pi_{\theta}</span> 表示参数为 <span class="math">\theta</span> 的策略，
而 <span class="math">J(\pi_{\theta})</span> 表示有限视野无折扣的策略回报期望。
<span class="math">J(\pi_{\theta})</span> 的梯度为</p>
<div class="math">
<p><span class="math">\nabla_{\theta} J(\pi_{\theta}) = \underE{\tau \sim \pi_{\theta}}{
    \sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t|s_t) A^{\pi_{\theta}}(s_t,a_t)
    },</span></p>
</div><p>其中 <span class="math">\tau</span> 是轨迹，<span class="math">A^{\pi_{\theta}}</span> 是当前策略的优势函数。</p>
<p>策略梯度算法的工作原理是通过随机梯度提升策略性能来更新策略参数：</p>
<div class="math">
<p><span class="math">\theta_{k+1} = \theta_k + \alpha \nabla_{\theta} J(\pi_{\theta_k})</span></p>
</div><p>策略梯度实现通常基于无限视野折扣回报来计算优势函数估计，尽管其他情况下使用有限视野无折扣策略梯度公式。</p>
</div>
<div class="section" id="id6">
<h3><a class="toc-backref" href="#id18">探索与利用</a><a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>VPG以在轨策略方式训练随机策略。这意味着它会根据其随机策略的最新版本通过采样操作来进行探索。
动作选择的随机性取决于初始条件和训练程序。在训练过程中，由于更新规则鼓励该策略利用已经发现的奖励，因此该策略通常变得越来越少随机性。
这可能会导致策略陷入局部最优状态。</p>
</div>
<div class="section" id="id7">
<h3><a class="toc-backref" href="#id19">伪代码</a><a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<div class="math">
<p><span class="math">\begin{algorithm}[H]
    \caption{Vanilla Policy Gradient Algorithm}
    \label{alg1}
\begin{algorithmic}[1]
    \STATE Input: initial policy parameters $\theta_0$, initial value function parameters $\phi_0$
    \FOR{$k = 0,1,2,...$}
    \STATE Collect set of trajectories ${\mathcal D}_k = \{\tau_i\}$ by running policy $\pi_k = \pi(\theta_k)$ in the environment.
    \STATE Compute rewards-to-go $\hat{R}_t$.
    \STATE Compute advantage estimates, $\hat{A}_t$ (using any method of advantage estimation) based on the current value function $V_{\phi_k}$.
    \STATE Estimate policy gradient as
        \begin{equation*}
        \hat{g}_k = \frac{1}{|{\mathcal D}_k|} \sum_{\tau \in {\mathcal D}_k} \sum_{t=0}^T \left. \nabla_{\theta} \log\pi_{\theta}(a_t|s_t)\right|_{\theta_k} \hat{A}_t.
        \end{equation*}
    \STATE Compute policy update, either using standard gradient ascent,
        \begin{equation*}
        \theta_{k+1} = \theta_k + \alpha_k \hat{g}_k,
        \end{equation*}
        or via another gradient ascent algorithm like Adam.
    \STATE Fit value function by regression on mean-squared error:
        \begin{equation*}
        \phi_{k+1} = \arg \min_{\phi} \frac{1}{|{\mathcal D}_k| T} \sum_{\tau \in {\mathcal D}_k} \sum_{t=0}^T\left( V_{\phi} (s_t) - \hat{R}_t \right)^2,
        \end{equation*}
        typically via some gradient descent algorithm.
    \ENDFOR
\end{algorithmic}
\end{algorithm}</span></p>
</div></div>
</div>
<div class="section" id="id8">
<h2><a class="toc-backref" href="#id20">文档</a><a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="spinup.vpg">
<code class="descclassname">spinup.</code><code class="descname">vpg</code><span class="sig-paren">(</span><em>env_fn</em>, <em>actor_critic=&lt;function mlp_actor_critic&gt;</em>, <em>ac_kwargs={}</em>, <em>seed=0</em>, <em>steps_per_epoch=4000</em>, <em>epochs=50</em>, <em>gamma=0.99</em>, <em>pi_lr=0.0003</em>, <em>vf_lr=0.001</em>, <em>train_v_iters=80</em>, <em>lam=0.97</em>, <em>max_ep_len=1000</em>, <em>logger_kwargs={}</em>, <em>save_freq=10</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/spinup/algos/vpg/vpg.html#vpg"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#spinup.vpg" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>env_fn</strong> &#8211; A function which creates a copy of the environment.
The environment must satisfy the OpenAI Gym API.</li>
<li><strong>actor_critic</strong> &#8211; <p>A function which takes in placeholder symbols
for state, <code class="docutils literal"><span class="pre">x_ph</span></code>, and action, <code class="docutils literal"><span class="pre">a_ph</span></code>, and returns the main
outputs from the agent&#8217;s Tensorflow computation graph:</p>
<table border="1" class="docutils">
<colgroup>
<col width="16%" />
<col width="24%" />
<col width="60%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Symbol</th>
<th class="head">Shape</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">pi</span></code></td>
<td>(batch, act_dim)</td>
<td><div class="first last line-block">
<div class="line">Samples actions from policy given</div>
<div class="line">states.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">logp</span></code></td>
<td>(batch,)</td>
<td><div class="first last line-block">
<div class="line">Gives log probability, according to</div>
<div class="line">the policy, of taking actions <code class="docutils literal"><span class="pre">a_ph</span></code></div>
<div class="line">in states <code class="docutils literal"><span class="pre">x_ph</span></code>.</div>
</div>
</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">logp_pi</span></code></td>
<td>(batch,)</td>
<td><div class="first last line-block">
<div class="line">Gives log probability, according to</div>
<div class="line">the policy, of the action sampled by</div>
<div class="line"><code class="docutils literal"><span class="pre">pi</span></code>.</div>
</div>
</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">v</span></code></td>
<td>(batch,)</td>
<td><div class="first last line-block">
<div class="line">Gives the value estimate for states</div>
<div class="line">in <code class="docutils literal"><span class="pre">x_ph</span></code>. (Critical: make sure</div>
<div class="line">to flatten this!)</div>
</div>
</td>
</tr>
</tbody>
</table>
</li>
<li><strong>ac_kwargs</strong> (<em>dict</em>) &#8211; Any kwargs appropriate for the actor_critic
function you provided to VPG.</li>
<li><strong>seed</strong> (<em>int</em>) &#8211; Seed for random number generators.</li>
<li><strong>steps_per_epoch</strong> (<em>int</em>) &#8211; Number of steps of interaction (state-action pairs)
for the agent and the environment in each epoch.</li>
<li><strong>epochs</strong> (<em>int</em>) &#8211; Number of epochs of interaction (equivalent to
number of policy updates) to perform.</li>
<li><strong>gamma</strong> (<em>float</em>) &#8211; Discount factor. (Always between 0 and 1.)</li>
<li><strong>pi_lr</strong> (<em>float</em>) &#8211; Learning rate for policy optimizer.</li>
<li><strong>vf_lr</strong> (<em>float</em>) &#8211; Learning rate for value function optimizer.</li>
<li><strong>train_v_iters</strong> (<em>int</em>) &#8211; Number of gradient descent steps to take on
value function per epoch.</li>
<li><strong>lam</strong> (<em>float</em>) &#8211; Lambda for GAE-Lambda. (Always between 0 and 1,
close to 1.)</li>
<li><strong>max_ep_len</strong> (<em>int</em>) &#8211; Maximum length of trajectory / episode / rollout.</li>
<li><strong>logger_kwargs</strong> (<em>dict</em>) &#8211; Keyword args for EpochLogger.</li>
<li><strong>save_freq</strong> (<em>int</em>) &#8211; How often (in terms of gap between epochs) to save
the current policy and value function.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<div class="section" id="id9">
<h3><a class="toc-backref" href="#id21">保存的模型的内容</a><a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
<p>记录的计算图包括：</p>
<table border="1" class="docutils">
<colgroup>
<col width="11%" />
<col width="89%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">键</th>
<th class="head">值</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal"><span class="pre">x</span></code></td>
<td>Tensorflow placeholder for state input.</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal"><span class="pre">pi</span></code></td>
<td>Samples an action from the agent, conditioned on states in <code class="docutils literal"><span class="pre">x</span></code>.</td>
</tr>
<tr class="row-even"><td><code class="docutils literal"><span class="pre">v</span></code></td>
<td>Gives value estimate for states in <code class="docutils literal"><span class="pre">x</span></code>.</td>
</tr>
</tbody>
</table>
<p>可以通过以下方式访问此保存的模型</p>
<ul class="simple">
<li>使用 <a class="reference external" href="../user/saving_and_loading.html#loading-and-running-trained-policies">test_policy.py</a> 工具运行经过训练的策略，</li>
<li>或使用 <a class="reference external" href="../utils/logger.html#spinup.utils.logx.restore_tf_graph">restore_tf_graph</a> 将整个保存的图形加载到程序中。</li>
</ul>
</div>
</div>
<div class="section" id="id10">
<h2><a class="toc-backref" href="#id22">参考</a><a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id11">
<h3><a class="toc-backref" href="#id23">相关论文</a><a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf">Policy Gradient Methods for Reinforcement Learning with Function Approximation</a>, Sutton et al. 2000</li>
<li><a class="reference external" href="http://joschu.net/docs/thesis.pdf">Optimizing Expectations: From Deep Reinforcement Learning to Stochastic Computation Graphs</a>, Schulman 2016(a)</li>
<li><a class="reference external" href="https://arxiv.org/abs/1604.06778">Benchmarking Deep Reinforcement Learning for Continuous Control</a>, Duan et al. 2016</li>
<li><a class="reference external" href="https://arxiv.org/abs/1506.02438">High Dimensional Continuous Control Using Generalized Advantage Estimation</a>, Schulman et al. 2016(b)</li>
</ul>
</div>
<div class="section" id="id12">
<h3><a class="toc-backref" href="#id24">为什么是这些论文？</a><a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h3>
<p>包含Sutton 2000是因为它是强化学习理论的永恒经典，并且包含了导致现代策略梯度的早期工作的参考。
之所以包括Schulman 2016（a），是因为第2章对策略梯度算法（包括伪代码）的理论进行了清晰的介绍。
Duan 2016是一份清晰的，最新的基准论文，显示了深度强化学习设置
（例如，以神经网络策略和Adam为优化器）中的vanilla policy gradient与其他深度强化算法的比较。
之所以包含Schulman 2016（b），是因为我们在VPG的实现中利用了
通用优势估计（Generalized Advantage Estimation）来计算策略梯度。</p>
</div>
<div class="section" id="id13">
<h3><a class="toc-backref" href="#id25">其他公开实现</a><a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><a class="reference external" href="https://github.com/rll/rllab/blob/master/rllab/algos/vpg.py">rllab</a></li>
<li><a class="reference external" href="https://github.com/ray-project/ray/blob/master/python/ray/rllib/agents/pg">rllib (Ray)</a></li>
</ul>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="trpo.html" class="btn btn-neutral float-right" title="Trust Region Policy Optimization" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../spinningup/bench.html" class="btn btn-neutral" title="Spinning Up 算法实现的基准" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, OpenAI.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-46660488-7"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-46660488-7');
</script>



</body>
</html>